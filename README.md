# Description of folders:
## Animator:
This contains the Unity project used to animate the 3D character using a CSV-file containing the joint locations. Before animating, make sure your CSV-file and audio file are already imported as assets in the Unity project folder.

* To animate the character, open the project in Unity and select the "Pianist" object in the Hierarchy window. The "animateBody" script should be attached to the object and shown in the inspector window on the right. Drag your CSV-file containing the motion capture data into the corresponding field. The script will distinguish whether the CSV-file came from the original dataset or if it was generated by the model and act accordingly. Drag the corresponding audio file into the AudioClip field below. 
* To record a video, use the Recorder plugin in the bottom window. Make sure the target framerate matches that of the dataset (100 fps for our data).

## Model:
This folder contains the Python code for running the model and other useful scripts. Please drop the piano dataset (https://gitlab.doc.gold.ac.uk/expressive-musical-gestures/dataset) folder in this folder as well.

### MELODIA:
This folder contains the Melodia script used for getting the Pitch from the dataset.

### analysis.py
This script can be used to automatically get the average positioning error, velocity, acceleration and jerk from the generated data for multiple feature combinations at once, as long as the folders for each condition are named accordingly:
* Pitch = onlyPitch; Pitch+RMS = withRMS; Pitch+Beat = withBeat; Pitch+RMS+Beat = pitchRMSbeat; MFCC = mfcc; and ALL = all.

### getDataset.py
This script can be used to generate datasets with the given expressive variations. It runs through all of the given folders and computes the needed features for each mocap/audio combination it encounters.

The generated dataset is as follows for every expressive variation combination: [[MoCap[0:42]], [Features: MFCC[0:16], MFCC-Delta[16:32], Pitch[32], Beat-Onsets[33], RMS[34]]]

### model.py
This script contains the LSTM network and the data preprocessing.

* Line 31 is used to shuffle the data such that it is no longer in alphabetical order, using a seed (make sure this seed is the same for each test run you do on the same dataset for more accurate comparisons). 
* The code between lines 55-68 can be altered in order to use different feature combinations (options listed in comments). After this, a new feature vector is made for each data sample. 
* For a better performance, you should handpick the test set using line 78. Alternatively, lines 78-93 can be commented out and lines 102-104 can be duplicated and altered to make a test set from a split of the training set. 
* Lines 107-161 are used to split the data in frames for online training. The length of these frames can be altered by using line 99 (100 fps: 1 sec = 100 steps).
* You can set the learning rate manually by using line 169, or uncomment lines 171-217 to run cyclical learning rates in order to find the best learning rate. 
* Training the model is done in lines 227-307, testing is done in lines 329-355. You can set the location for the generated results in line 354. 
